{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Image.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Image.py\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import Input, layers\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "import streamlit as st\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from pickle import dump, load\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "max_length=34\n",
    "\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "\t# load document\n",
    "\tdoc = load_doc(filename)\n",
    "\tdescriptions = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\t# split id from description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# skip images not in the set\n",
    "\t\tif image_id in dataset:\n",
    "\t\t\t# create list\n",
    "\t\t\tif image_id not in descriptions:\n",
    "\t\t\t\tdescriptions[image_id] = list()\n",
    "\t\t\t# wrap description in tokens\n",
    "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "\t\t\t# store\n",
    "\t\t\tdescriptions[image_id].append(desc)\n",
    "\treturn descriptions\n",
    "\n",
    "def load_set(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdataset = list()\n",
    "\t# process line by line\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# skip empty lines\n",
    "\t\tif len(line) < 1:\n",
    "\t\t\tcontinue\n",
    "\t\t# get the image identifier\n",
    "\t\tidentifier = line.split('.')[0]\n",
    "\t\tdataset.append(identifier)\n",
    "\treturn set(dataset)\n",
    "\n",
    "train = load_set('C:/Users/Laxmi/Desktop/Flickr Dataset/Flickr dataset/Flickr_8k.trainImages.txt')\n",
    "\n",
    "train_descriptions = load_clean_descriptions(\n",
    "    'C:/Users/Laxmi/Desktop/Flickr Dataset/Flickr dataset/descriptions.txt', train)\n",
    "\n",
    "all_train_captions = []\n",
    "for key, val in train_descriptions.items():\n",
    "    for cap in val:\n",
    "        all_train_captions.append(cap)\n",
    "\n",
    "word_count_threshold = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for sent in all_train_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "\n",
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoix[w] = ix\n",
    "    ixtoword[ix] = w\n",
    "    ix += 1\n",
    "    \n",
    "vocab_size = len(ixtoword) + 1\n",
    "\n",
    "\n",
    "def greedySearch(model, photo):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        photo = np.resize(photo, (1,2048))\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        if word == 'None':\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final\n",
    "\n",
    "\n",
    "\n",
    "def get_predictions():\n",
    "    \n",
    "    \n",
    "    model = tf.keras.models.load_model('C:/Users/Laxmi/Desktop/Image captioning/model_4.h5')\n",
    "    model.load_weights('C:/Users/Laxmi/Desktop/Image captioning/model_4.h5')\n",
    "       \n",
    "    images = 'C:/Users/Laxmi/Desktop/Flickr Dataset/Flicker8k_Dataset_Image/'\n",
    "    \n",
    "    with open(\"C:/Users/Laxmi/Desktop/Flickr Dataset/Flickr dataset/encoded_test_images.pkl\", \"rb\") as encoded_pickle:\n",
    "        encoding_test = load(encoded_pickle)\n",
    "        index = np.random.choice(1000)\n",
    "        pic = list(encoding_test.keys())[index] \n",
    "        image = encoding_test[pic].resize((1,2048))\n",
    "        x=plt.imread(images+pic)\n",
    "        #np.resize(x, (-1, 200, 350))\n",
    "        st.sidebar.image(x,use_column_width=True)     \n",
    "        caption = greedySearch(model, x)\n",
    "    st.write(caption)\n",
    "        \n",
    "\n",
    "st.title('Image caption generator')\n",
    "st.sidebar.markdown('## Input Image')\n",
    "if st.button('Generate a Random Image'):\n",
    "    get_predictions()\n",
    "    \n",
    "st.write('## Caption Generated is: ')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
